### Welcome to the complete DSA guide by Mihraz Hossain (for batch 2023)
If you can finish this, I would like to congratulate you cause you will be ready to take the exam!

SO Best wishes and we will be **starting with:**

# üìò LECTURE 2 ‚Äî Math Foundations (Sets, Relations, Recursion)

üëâ **Reality check:**
This lecture is **easy marks**. No proofs in exam. Mostly definitions + logic.

## 1Ô∏è‚É£ SETS (VERY IMPORTANT)

### What is a set?

A **set** is a collection of **distinct** elements.

Examples:

* `{1, 2, 3}`
* `{apple, pear}`
* `{(1,2), (3,4)}`

üìå **Order does NOT matter**
`{1,2,3} = {3,2,1}`


### Membership symbol (‚àà)

* `x ‚àà A` ‚Üí x is an element of A
* `x ‚àâ A` ‚Üí x is NOT an element of A

Example:

* `2 ‚àà {1,2,3}` ‚úÖ
* `5 ‚àâ {1,2,3}` ‚ùå


### Subset (‚äÜ) ‚≠ê EXAM FAVORITE

A is a subset of B if **every element of A is in B**

Example:

* `{1,2} ‚äÜ {1,2,3}` ‚úÖ
* `{1,4} ‚äÜ {1,2,3}` ‚ùå

### Proper Subset (‚äÇ)

* A ‚äÇ B means:

  * A ‚äÜ B **and**
  * A ‚â† B

Example:

* `{1,2} ‚äÇ {1,2,3}` ‚úÖ
* `{1,2,3} ‚äÇ {1,2,3}` ‚ùå


## 2Ô∏è‚É£ VERY IMPORTANT DIFFERENCE: ‚àà vs ‚äÜ

This **will** be tested.

Let:

* `A = {1,2}`

| Statement | True/False |
|  | - |
| `1 ‚àà A`   | ‚úÖ          |
| `{1} ‚äÜ A` | ‚úÖ          |
| `1 ‚äÜ A`   | ‚ùå          |
| `{1} ‚àà A` | ‚ùå          |

üëâ **Rule to remember**:

* `‚àà` ‚Üí element
* `‚äÜ` ‚Üí set


## 3Ô∏è‚É£ EMPTY SET (‚àÖ)

* Empty set has **no elements**
* `{}` or `‚àÖ`

Important facts:

* `‚àÖ ‚äÜ A` ‚Üí ALWAYS true
* `‚àÖ ‚àà A` ‚Üí usually false (unless A contains ‚àÖ)


## 4Ô∏è‚É£ POWER SET P(S) ‚≠ê‚≠ê‚≠ê

### Definition:

Power set = **set of all subsets**

If:

* `S = {1,2}`

Then:

```
P(S) = { ‚àÖ, {1}, {2}, {1,2} }
```

### GOLDEN RULE (MEMORIZE):

If a set has **n elements**,
üëâ **Power set has `2^n` elements**

Example:

* `{1,2,3}` ‚Üí `2¬≥ = 8` subsets



## 5Ô∏è‚É£ RELATIONS & FUNCTIONS (LIGHT EXAM WEIGHT)

### Relation:

A relation is a **set of ordered pairs**

Example:

```
R = {(1,2), (2,3)}
```

No heavy theory required.



### Function:

A function maps **each input to exactly ONE output**

Example:

```
f(x) = x + 1
```


## 6Ô∏è‚É£ RECURSION (NO PROOFS)

### What is recursion?

A function that **calls itself**

Example:

```
Factorial(n):
if n == 0 return 1
else return n * factorial(n-1)
```

### What exam may ask:

* Trace recursion
* Count calls
* Understand base case


## üìù EXAM-STYLE QUESTIONS (FROM THIS LECTURE)

1. Is `{1,2} ‚äÜ {1,2,3}`? Explain.
2. How many subsets does `{a,b,c}` have?
3. Is `{1}` ‚àà `{1,2,3}`?
4. What is the difference between ‚àà and ‚äÜ?
5. Write the power set of `{1,2}`.



## ‚úÖ QUICK SELF-CHECK (ANSWER IN YOUR HEAD)

* Is `‚àÖ ‚äÜ {1,2}`? ‚Üí YES
* Is `{1} ‚àà {1,2}`? ‚Üí NO
* Power set size of `{a,b,c,d}`? ‚Üí 16


# üìò LECTURE 3 ‚Äî Algorithm Analysis (Big-O, Time Complexity)

üëâ **Why this lecture matters**

* Comes in **theory questions**
* Comes in **coding questions**
* Comes in **project explanations**
* Easy marks if you know the patterns

## 1Ô∏è‚É£ Why do we analyze algorithms?

### Core idea (exam sentence)

> Algorithm analysis estimates how an algorithm‚Äôs **time or memory usage grows as input size increases**, independent of machine or language.

We care about:

* **Time complexity**
* **Space complexity**

In CS311, **time complexity is more important**.



## 2Ô∏è‚É£ Input size (n)

* `n` = number of elements
* Could be:

  * array size
  * number of nodes
  * number of inputs

üìå **Almost every formula uses n**



## 3Ô∏è‚É£ What is Big-O notation? ‚≠ê‚≠ê‚≠ê

### Big-O = **upper bound**

It tells us the **worst-case growth rate**.

üëâ Exam sentence:

> Big-O describes the maximum running time of an algorithm as input size grows.



### The three notations (know names + meaning)

| Notation    | Meaning                    |
| -- | -- |
| **O(f(n))** | Upper bound (worst case)   |
| **Œò(f(n))** | Tight bound (exact growth) |
| **Œ©(f(n))** | Lower bound (best case)    |

üìå If confused ‚Üí **Big-O is the most important**



## 4Ô∏è‚É£ Growth rates (MEMORIZE THIS TABLE)

| Name         | Complexity |
|  | - |
| Constant     | O(1)       |
| Logarithmic  | O(log n)   |
| Linear       | O(n)       |
| Linearithmic | O(n log n) |
| Quadratic    | O(n¬≤)      |
| Cubic        | O(n¬≥)      |
| Exponential  | O(2‚Åø)      |

üëâ **Order from fastest to slowest**:

```
O(1) < O(log n) < O(n) < O(n log n) < O(n¬≤) < O(2‚Åø)
```



## 5Ô∏è‚É£ How to find Big-O of code (EXAM CORE)

### Rule 1: Ignore constants

```
O(2n + 5) ‚Üí O(n)
```



### Rule 2: Ignore smaller terms

```
O(n¬≤ + n) ‚Üí O(n¬≤)
```



### Rule 3: Loops

#### Single loop

```c
for(i = 0; i < n; i++)
```

‚û° **O(n)**



#### Nested loops

```c
for(i = 0; i < n; i++)
  for(j = 0; j < n; j++)
```

‚û° **O(n¬≤)**



#### Nested but dependent

```c
for(i = 0; i < n; i++)
  for(j = 0; j < i; j++)
```

‚û° still **O(n¬≤)**



## 6Ô∏è‚É£ Logarithmic time ‚≠ê VERY IMPORTANT

### Binary Search

* Each step halves input
* Number of steps ‚âà log‚ÇÇ(n)

üëâ **O(log n)**

Example:

```
n = 16 ‚Üí 4 steps
n = 32 ‚Üí 5 steps
```

üìå Exam sentence:

> Binary search is faster than linear search because it reduces the search space by half each step.



## 7Ô∏è‚É£ Recursive algorithms

### Example: Factorial

```
T(n) = T(n-1) + c
```

‚û° **O(n)**



### Fibonacci (naive recursion)

```
F(n) = F(n-1) + F(n-2)
```

‚û° **O(2‚Åø)** ‚ùå slow

üëâ Why?

* Repeated calculations
* Overlapping subproblems

(This connects to **Lecture 12 ‚Äì Dynamic Programming**)



## 8Ô∏è‚É£ Typical exam Big-O questions (VERY LIKELY)

### Example 1

```c
for(i = 0; i < n; i++)
  printf("Hello");
```

‚úî **O(n)**



### Example 2

```c
for(i = 0; i < n; i++)
  for(j = 0; j < n; j++)
    printf("Hello");
```

‚úî **O(n¬≤)**



### Example 3

```c
while(n > 1)
  n = n / 2;
```

‚úî **O(log n)**



### Example 4

```c
for(i = 0; i < n; i++)
  for(j = 0; j < i; j++)
    for(k = 0; k < 10; k++)
```

‚úî **O(n¬≤)**
(10 is constant ‚Üí ignored)



## 9Ô∏è‚É£ Why faster algorithms matter (THEORY QUESTION)

Example:

* Linear search ‚Üí O(n)
* Binary search ‚Üí O(log n)

For large n:

* O(log n) is **much faster**
* That‚Äôs why sorting + binary search is useful



## üìù EXAM-STYLE QUESTIONS FROM THIS LECTURE

1. Define Big-O notation.
2. Find time complexity of a given code.
3. Why binary search is faster than linear search?
4. What is the time complexity of merge sort?
5. Why naive Fibonacci recursion is inefficient?



## ‚úÖ QUICK SELF-CHECK (IMPORTANT)

Answer in your head:

* Single loop ‚Üí ?
* Nested loop ‚Üí ?
* Binary search ‚Üí ?
* Ignore constants? (Yes/No)
* O(n¬≤ + n)? ‚Üí ?

Answers:

> O(n), O(n¬≤), O(log n), Yes, O(n¬≤)


# üìò LECTURE 4 ‚Äî Lists & Abstract Data Types (ADT)

üëâ **Big idea**
Lecture 4 teaches you **how data is organized** and **why ADTs matter**.



## 1Ô∏è‚É£ What is a Data Structure?

### Simple definition (exam sentence):

> A data structure is a way to store and organize data so that it can be used efficiently.

Examples:

* Array
* List
* Stack
* Queue
* Tree



## 2Ô∏è‚É£ Abstract Data Type (ADT) ‚≠ê‚≠ê‚≠ê

### Definition (VERY IMPORTANT):

> An ADT defines **what operations can be performed**, not **how they are implemented**.

üìå Think of it like a **remote control**:

* You know what buttons do
* You don‚Äôt care how TV works internally



### Why ADT is useful?

* Hides implementation details
* Easier to change implementation later
* Reduces program complexity

üëâ **Exam question**:
‚ÄúWhy ADT is important in software design?‚Äù



## 3Ô∏è‚É£ What is a List?

### Definition:

> A list is a **finite ordered sequence** of elements.

Example:

```
<10, 20, 30, 40>
```

Important:

* Order **matters**
* Each element has a **position**



## 4Ô∏è‚É£ Basic List Operations (MEMORIZE)

Common operations:

* Create
* Insert
* Delete
* Find/Search
* Get length
* Traverse (move next / previous)

üëâ These operations are the **list ADT interface**



## 5Ô∏è‚É£ Two Types of List Implementations ‚≠ê‚≠ê‚≠ê

### 1. Array-Based List

Stored in **continuous memory**

#### Advantages:

* Fast access by index ‚Üí **O(1)**
* Simple

#### Disadvantages:

* Fixed size
* Insert/delete in middle ‚Üí **O(n)**



### 2. Linked List

Stored as **nodes connected by pointers**

Each node:

```
[data | next]
```

#### Advantages:

* Dynamic size
* Insert/delete ‚Üí **O(1)** (if position known)

#### Disadvantages:

* Slow access ‚Üí **O(n)**
* Extra memory for pointers



## 6Ô∏è‚É£ Array vs Linked List (VERY IMPORTANT TABLE)

| Feature       | Array List | Linked List    |
| - | - | -- |
| Memory        | Contiguous | Non-contiguous |
| Access        | O(1)       | O(n)           |
| Insert/Delete | O(n)       | O(1)           |
| Size          | Fixed      | Dynamic        |
| Extra Memory  | No         | Yes (pointers) |

üëâ **Exam loves comparison questions**



## 7Ô∏è‚É£ When to use which?

### Use Array List when:

* Frequent access
* Size known
* Few insertions

### Use Linked List when:

* Frequent insert/delete
* Size changes
* Order matters but random access doesn‚Äôt



## 8Ô∏è‚É£ Traversing a List

Example (pseudo):

```c
for (moveToStart; currPos < length; next)
  process(currentElement)
```

üëâ Time complexity: **O(n)**



## 9Ô∏è‚É£ Typical Exam Questions (Lecture 4)

1. What is an ADT?
2. Difference between list and array?
3. Compare array-based list and linked list.
4. What operations does a list ADT support?
5. Why linked list insertion is faster?



## ‚úÖ QUICK SELF-CHECK

Answer quickly:

* Does list allow duplicates? ‚Üí YES
* Does order matter? ‚Üí YES
* Random access in linked list? ‚Üí NO
* Insert in array list cost? ‚Üí O(n)


# üìò LECTURE 5 ‚Äî Stacks & Queues

üëâ **Big idea**
Stacks and Queues are **restricted lists** ‚Äî fewer operations, easier logic, very useful.


## 1Ô∏è‚É£ STACK ‚≠ê‚≠ê‚≠ê (VERY IMPORTANT)

### Definition (EXAM SENTENCE)

> A stack is a linear data structure where insertion and deletion occur only at one end called the **top**.

### Rule:

**LIFO ‚Äî Last In, First Out**



## 2Ô∏è‚É£ Stack Operations (MEMORIZE)

| Operation  | Meaning         |
| - |  |
| PUSH       | Insert element  |
| POP        | Remove element  |
| TOP / PEEK | See top element |
| isEmpty    | Check empty     |

Example:

```
PUSH 10
PUSH 20
PUSH 30
POP ‚Üí 30
```



## 3Ô∏è‚É£ Stack Implementation

### Using Array

* Simple
* Fixed size
* Possible overflow

### Using Linked List

* Dynamic
* No overflow (until memory ends)

üëâ **Exam question**:
‚ÄúImplement stack using array or linked list (conceptually)‚Äù



## 4Ô∏è‚É£ Stack Applications ‚≠ê‚≠ê‚≠ê (EXAM FAVORITE)

### 1. Function calls (Recursion)

* Each function call is pushed onto stack
* Return pops it



### 2. Expression evaluation

* Infix ‚Üí Postfix
* Postfix evaluation



### 3. Bracket Matching ‚≠ê‚≠ê‚≠ê (VERY LIKELY)

Example:

```
{ [ ( ) ] }
```

Algorithm (memorize):

1. Create empty stack
2. Read characters one by one
3. If opening bracket ‚Üí PUSH
4. If closing bracket ‚Üí POP and match
5. Stack empty at end ‚Üí Balanced

üëâ **Coding question possible**



## 5Ô∏è‚É£ QUEUE ‚≠ê‚≠ê‚≠ê

### Definition (EXAM SENTENCE)

> A queue is a linear data structure where insertion occurs at the **rear** and deletion occurs at the **front**.

### Rule:

**FIFO ‚Äî First In, First Out**



## 6Ô∏è‚É£ Queue Operations

| Operation | Meaning       |
|  | - |
| Enqueue   | Insert        |
| Dequeue   | Remove        |
| Front     | First element |
| Rear      | Last element  |

Example:

```
Enqueue 10
Enqueue 20
Dequeue ‚Üí 10
```



## 7Ô∏è‚É£ Queue Implementation

### Array Queue

* Simple
* Wasted space issue

### Circular Queue ‚≠ê

* Rear wraps around
* Efficient memory usage



### Linked List Queue

* Dynamic
* Efficient enqueue/dequeue



## 8Ô∏è‚É£ Queue Applications

* CPU scheduling
* Printer queue
* BFS traversal (Graphs ‚Äî later)



## 9Ô∏è‚É£ Stack vs Queue (VERY IMPORTANT)

| Feature    | Stack   | Queue    |
| - | - | -- |
| Rule       | LIFO    | FIFO     |
| Insert     | PUSH    | Enqueue  |
| Remove     | POP     | Dequeue  |
| Access end | One end | Two ends |

üëâ **Guaranteed comparison question**



## üî¢ Time Complexity (Easy Marks)

| Operation | Stack | Queue |
|  | -- | -- |
| Insert    | O(1)  | O(1)  |
| Delete    | O(1)  | O(1)  |



## üìù EXAM-STYLE QUESTIONS

1. Define stack and queue.
2. Explain LIFO and FIFO.
3. Stack vs Queue.
4. Explain bracket matching using stack.
5. Applications of stack and queue.



## üíª VERY LIKELY CODING QUESTION

### Bracket Matching (Pseudo-code idea)

```c
for each char in string:
  if opening bracket:
    push
  else if closing bracket:
    if stack empty ‚Üí unbalanced
    pop
end
if stack empty ‚Üí balanced
```



## ‚úÖ QUICK SELF-CHECK

Answer in your head:

* Stack rule? ‚Üí LIFO
* Queue rule? ‚Üí FIFO
* Which uses recursion? ‚Üí Stack
* BFS uses? ‚Üí Queue

# üå≥ LECTURE 7 ‚Äî TREES

üëâ **Big idea**
Trees are used when we want **fast searching + structured data** (better than lists).


## 1Ô∏è‚É£ What is a Tree?

### Definition (EXAM SENTENCE)

> A tree is a hierarchical data structure consisting of **nodes** connected by **edges**, with one node designated as the **root**.



## 2Ô∏è‚É£ Tree Terminology ‚≠ê‚≠ê‚≠ê (MEMORIZE)

| Term          | Meaning                  |
| - |  |
| Node          | Data element             |
| Edge          | Connection between nodes |
| Root          | Top node                 |
| Parent        | Node above               |
| Child         | Node below               |
| Leaf          | Node with no children    |
| Internal node | Node with children       |
| Subtree       | Tree formed from a node  |

üëâ **Exam loves terminology questions**



## 3Ô∏è‚É£ Depth, Level, Height (VERY IMPORTANT)

### Depth of a node

* Number of edges from **root to node**

### Level of a node

* Root is **level 0**

### Height of a tree

* Maximum level in the tree

üìå **Key relationship**:

> Height measures the **longest path** from root to a leaf.



## 4Ô∏è‚É£ Binary Tree ‚≠ê‚≠ê‚≠ê

### Definition:

> A binary tree is a tree in which **each node has at most two children**.

Children are:

* Left child
* Right child



## 5Ô∏è‚É£ Types of Binary Trees (EXAM FAVORITE)

### 1. Full Binary Tree

* Every node has **0 or 2 children**



### 2. Complete Binary Tree

* All levels filled **except possibly last**
* Last level filled **from left to right**



### 3. Perfect Binary Tree

* Full + Complete
* All levels completely filled

üëâ **Exam question**:
‚ÄúDifferentiate full, complete, and perfect binary trees.‚Äù



## 6Ô∏è‚É£ Binary Tree Properties ‚≠ê‚≠ê‚≠ê

For a **perfect binary tree** of height `h`:

* Number of nodes = `2^h ‚àí 1`

üëâ This formula is often asked.



## 7Ô∏è‚É£ Tree Traversals ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (VERY IMPORTANT)

Traversal = visiting all nodes **once**.

### 1. Preorder

```
Root ‚Üí Left ‚Üí Right
```



### 2. Inorder

```
Left ‚Üí Root ‚Üí Right
```



### 3. Postorder

```
Left ‚Üí Right ‚Üí Root
```



### 4. Level Order

* Visit level by level
* Uses **queue**



### Example (mentally visualize):

```
      A
     / \
    B   C
   / \
  D   E
```

| Traversal   | Result    |
| -- |  |
| Preorder    | A B D E C |
| Inorder     | D B E A C |
| Postorder   | D E B C A |
| Level order | A B C D E |

üëâ **Guaranteed exam question**



## 8Ô∏è‚É£ Binary Search Tree (BST) ‚≠ê‚≠ê‚≠ê

### Definition (EXAM SENTENCE)

> A Binary Search Tree is a binary tree where:

* Left subtree values < root
* Right subtree values ‚â• root



### BST Properties

* Inorder traversal ‚Üí **sorted order**
* Average search ‚Üí **O(log n)**
* Worst case (skewed) ‚Üí **O(n)**



### Why BST is better than list?

* Faster search
* Structured insertion



## 9Ô∏è‚É£ Relationship Between Height & Performance ‚≠ê‚≠ê‚≠ê

| Tree shape | Height | Performance |
| - |  | -- |
| Balanced   | log n  | Fast        |
| Skewed     | n      | Slow        |

üëâ This explains **why balancing matters** (AVL, B-tree later).



## üîÅ Tree Traversal (Recursive Pseudo-code)

### Inorder traversal

```c
inorder(node):
  if node == NULL return
  inorder(node->left)
  visit(node)
  inorder(node->right)
```

üëâ Very likely **coding question**



## üìù EXAM-STYLE QUESTIONS

1. Define tree and binary tree.
2. Difference between full and complete binary tree.
3. Find inorder / preorder traversal of a given tree.
4. Why inorder traversal of BST is sorted?
5. What is height of a tree?



## ‚úÖ QUICK SELF-CHECK

Answer in your head:

* Max children in binary tree? ‚Üí 2
* Traversal that gives sorted BST? ‚Üí Inorder
* Which traversal uses queue? ‚Üí Level order
* Height of skewed tree with n nodes? ‚Üí n

If you got these ‚Üí excellent.

Excellent ‚Äî **Lecture 8 (Sorting)** is one of the **most important lectures for your exam**, especially because:

* Sorting appears in **HWs**
* Sorting appears in **projects**
* Sorting is **very likely one of the 2 coding questions**

We‚Äôll make this **clear, structured, and exam-safe**.



# üîÄ LECTURE 8 ‚Äî SORTING ALGORITHMS

üëâ **Big idea**
Sorting = arranging data in **non-decreasing order** based on a **key**.



## 1Ô∏è‚É£ Why Sorting Matters?

* Makes searching faster (binary search)
* Organizes data
* Used everywhere (databases, OS, algorithms)



## 2Ô∏è‚É£ Important Sorting Terms ‚≠ê

### Stable Sort

> A sorting algorithm is **stable** if it preserves the relative order of equal elements.

Example:

```
(5,a) (3,b) (5,c)
```

After stable sort:

```
(3,b) (5,a) (5,c)
```



### Internal vs External Sorting

* **Internal**: data fits in memory
* **External**: data too large ‚Üí disk-based

(Exam focus: **internal sorting**)



## 3Ô∏è‚É£ Three Simple Sorting Algorithms (O(n¬≤)) ‚≠ê‚≠ê‚≠ê

### 1. Insertion Sort ‚≠ê‚≠ê‚≠ê (VERY LIKELY CODE)

#### Idea:

* Insert element into already sorted part

Example:

```
[27 53] 36 ‚Üí [27 36 53]
```

#### Pseudo-code:

```c
for i = 1 to n-1
  for j = i down to 1
    if A[j] < A[j-1]
      swap
```

#### Time Complexity:

* Best: **O(n)** (already sorted)
* Avg/Worst: **O(n¬≤)**

‚úî Stable
‚úî Simple
‚úî Good for small datasets



### 2. Bubble Sort

#### Idea:

* Repeatedly swap adjacent elements

#### Time Complexity:

* Best: **O(n¬≤)**
* Worst: **O(n¬≤)**

‚ùå Slow
‚úî Simple



### 3. Selection Sort

#### Idea:

* Select smallest element and place at front

#### Time Complexity:

* Best/Worst: **O(n¬≤)**

‚ùå Not stable
‚úî Few swaps



## 4Ô∏è‚É£ Fast Sorting Algorithms ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê

### 4. Merge Sort ‚≠ê‚≠ê‚≠ê

#### Strategy:

**Divide and Conquer**

Steps:

1. Divide array into halves
2. Sort halves recursively
3. Merge sorted halves

#### Time Complexity:

* Best/Average/Worst: **O(n log n)**

‚úî Stable
‚ùå Extra memory needed



### 5. Quick Sort ‚≠ê‚≠ê‚≠ê (PROJECT RELEVANT)

#### Strategy:

* Choose pivot
* Partition array
* Recursively sort

#### Time Complexity:

* Average: **O(n log n)**
* Worst: **O(n¬≤)** (bad pivot)

‚úî Fast in practice
‚ùå Not stable

üëâ Your **Project 2** is from here.



## 5Ô∏è‚É£ Comparison Table (MEMORIZE THIS)

| Algorithm | Best       | Avg        | Worst      | Stable |
|  | - | - | - |  |
| Insertion | O(n)       | O(n¬≤)      | O(n¬≤)      | ‚úî      |
| Bubble    | O(n¬≤)      | O(n¬≤)      | O(n¬≤)      | ‚úî      |
| Selection | O(n¬≤)      | O(n¬≤)      | O(n¬≤)      | ‚ùå      |
| Merge     | O(n log n) | O(n log n) | O(n log n) | ‚úî      |
| Quick     | O(n log n) | O(n log n) | O(n¬≤)      | ‚ùå      |



## 6Ô∏è‚É£ When to Use Which? (THEORY QUESTION)

* Small input ‚Üí **Insertion sort**
* Large input ‚Üí **Merge / Quick**
* Need stability ‚Üí **Merge / Insertion**
* Memory limited ‚Üí **Quick**



## 7Ô∏è‚É£ VERY LIKELY CODING QUESTIONS

### A. Insertion Sort (WRITE THIS)

```c
void insertionSort(int A[], int n) {
  for(int i = 1; i < n; i++) {
    int key = A[i];
    int j = i - 1;
    while(j >= 0 && A[j] > key) {
      A[j + 1] = A[j];
      j--;
    }
    A[j + 1] = key;
  }
}
```



### B. Merge Sort (STRUCTURE)

```c
mergeSort(A, l, r):
  if l < r:
    m = (l + r) / 2
    mergeSort(A, l, m)
    mergeSort(A, m+1, r)
    merge(A, l, m, r)
```



## üìù EXAM-STYLE QUESTIONS

1. Compare insertion and selection sort.
2. Why quick sort is fast in practice?
3. Which sorting algorithm is stable?
4. Explain merge sort.
5. Write insertion sort code.



## ‚úÖ QUICK SELF-CHECK

Answer mentally:

* Best case of insertion sort? ‚Üí O(n)
* Stable fast algorithm? ‚Üí Merge sort
* Worst case of quick sort? ‚Üí O(n¬≤)
* Sorting used in Project 2? ‚Üí Quick sort



# üîç LECTURE 9 ‚Äî SEARCHING & HASHING

üëâ **Big idea**
Searching = finding a record with a given key.
Hashing = **direct access** using a hash function.



## PART A ‚Äî SEARCHING



## 1Ô∏è‚É£ What is Searching?

### Definition (EXAM SENTENCE)

> Searching is the process of locating a record with a specified key in a collection of data.



## 2Ô∏è‚É£ Linear (Sequential) Search ‚≠ê

### Idea:

* Check elements **one by one**

### Time Complexity:

* Best case: **O(1)**
* Worst case: **O(n)**
* Average case: **O(n)**

### Pseudo-code:

```c
for i = 0 to n-1
  if A[i] == key
    return i
return -1
```

‚úî Works on **unsorted data**
‚ùå Slow for large n



## 3Ô∏è‚É£ Binary Search ‚≠ê‚≠ê‚≠ê (VERY IMPORTANT)

### Condition:

‚ö†Ô∏è **Array must be sorted**

### Idea:

* Compare with middle element
* Eliminate half of data each step

### Time Complexity:

* **O(log n)**

### Pseudo-code (iterative):

```c
low = 0, high = n-1
while low <= high:
  mid = (low + high)/2
  if A[mid] == key ‚Üí found
  else if key < A[mid] ‚Üí high = mid-1
  else ‚Üí low = mid+1
```

üëâ **Exam sentence**:

> Binary search is faster than linear search because it reduces the search space by half at each step.



## 4Ô∏è‚É£ Comparing Linear vs Binary Search ‚≠ê‚≠ê‚≠ê

| Feature         | Linear     | Binary           |
|  | - | - |
| Sorted required | ‚ùå          | ‚úî                |
| Time            | O(n)       | O(log n)         |
| Method          | Sequential | Divide & conquer |



## PART B ‚Äî HASHING (VERY IMPORTANT)



## 5Ô∏è‚É£ What is Hashing?

### Definition (EXAM SENTENCE)

> Hashing is a technique that maps a key directly to an index in a hash table using a hash function.



## 6Ô∏è‚É£ Hash Table

* Array-like structure
* Stores key-value pairs
* Index computed using **hash function**

Example:

```
index = key % table_size
```



## 7Ô∏è‚É£ Why Hashing is Fast?

* Direct access
* Average time complexity:

  * Search: **O(1)**
  * Insert: **O(1)**
  * Delete: **O(1)**

üëâ **Exam question**:
‚ÄúWhy hashing is faster than searching?‚Äù



## 8Ô∏è‚É£ Collision ‚≠ê‚≠ê‚≠ê

### What is collision?

> When two different keys produce the same hash value.

Example:

```
h(12) = 2
h(22) = 2
```



## 9Ô∏è‚É£ Collision Handling Techniques ‚≠ê‚≠ê‚≠ê

### 1. Chaining

* Each table index has a linked list
* All collided elements go there

‚úî Simple
‚ùå Extra memory



### 2. Open Addressing

Store elements in the table itself.

Types:

* Linear probing
* Quadratic probing
* Double hashing

Example (Linear Probing):

```
h(key) = index
if occupied ‚Üí try index+1, index+2, ...
```



## 1Ô∏è‚É£0Ô∏è‚É£ Limitations of Hashing (EXAM THEORY)

* No range queries
* No order
* Depends on good hash function

üëâ That‚Äôs why **trees & indexing** are needed (Lecture 10).



## üìù EXAM-STYLE QUESTIONS

1. Define searching.
2. Difference between linear and binary search.
3. Why binary search requires sorted array?
4. What is hashing?
5. What is collision? How is it handled?
6. Compare chaining and open addressing.



## üíª LIKELY CODING QUESTION

### Binary Search (WRITE THIS)

```c
int binarySearch(int A[], int n, int key) {
  int low = 0, high = n - 1;
  while(low <= high) {
    int mid = (low + high) / 2;
    if(A[mid] == key) return mid;
    else if(key < A[mid]) high = mid - 1;
    else low = mid + 1;
  }
  return -1;
}
```



## ‚úÖ QUICK SELF-CHECK

Answer mentally:

* Binary search time? ‚Üí O(log n)
* Hashing average time? ‚Üí O(1)
* Collision meaning? ‚Üí Same hash index
* Chaining uses? ‚Üí Linked list


# üìò LECTURE 10 ‚Äî Indexing & Advanced Trees

üëâ **Big idea**
Hashing is fast, but limited.
Indexing and advanced trees solve **searching large data efficiently**, especially on disk.



## 1Ô∏è‚É£ Why Hashing is NOT Enough (EXAM FAVORITE)

Even though hashing is fast (**O(1)** average), it has **limitations**.

### Limitations of Hashing:

1. ‚ùå Cannot handle **range queries**

   * Example: find students with marks between 70‚Äì80
2. ‚ùå Cannot easily find **minimum / maximum**
3. ‚ùå No sorted order
4. ‚ùå Performance depends on hash function

üëâ **Exam sentence**:

> Hashing supports only exact-match queries and does not preserve order.



## 2Ô∏è‚É£ What is Indexing? ‚≠ê‚≠ê‚≠ê

### Definition (EXAM SENTENCE)

> An index is a data structure that improves the speed of data retrieval operations on large datasets.

Think of:

* **Book index**
* **Database index**



## 3Ô∏è‚É£ What Operations Does an Index Support?

| Operation    | Time      |
|  |  |
| Insert       | O(log n)  |
| Delete       | O(log n)  |
| Exact search | O(log n)  |
| Range search | Efficient |
| Min / Max    | O(log n)  |

üëâ Indexing is **better than hashing** for many applications.



## 4Ô∏è‚É£ Linear Index

### Idea:

* Sorted list of keys
* Each key points to data location

### Problem:

* Index becomes **too large**
* Slow disk access



## 5Ô∏è‚É£ Why Not Use a Simple BST? ‚≠ê‚≠ê‚≠ê

### Problems with BST:

1. May become **unbalanced**
2. Height can be **O(n)**
3. Too many disk accesses

üëâ Disk access is **VERY slow** ‚Üí we want **short trees**



## 6Ô∏è‚É£ B-Trees ‚≠ê‚≠ê‚≠ê (IMPORTANT)

### What is a B-tree?

> A B-tree is a **height-balanced multi-way tree** designed for disk storage.



### Key Properties (MEMORIZE):

* Always **balanced**
* All leaves at **same level**
* Each node can have **many children**
* Keys inside node are **sorted**

Example:

* Order `m`
* Each node has between `‚åàm/2‚åâ` and `m` children



## 7Ô∏è‚É£ Why B-Trees Are Used in Databases?

* Very **short height**
* Fewer disk accesses
* Efficient for large datasets

üëâ **Exam sentence**:

> B-trees reduce disk access by storing multiple keys in a single node.



## 8Ô∏è‚É£ B+ Trees (Mention Only)

Difference from B-tree:

* Data stored only in **leaf nodes**
* Internal nodes store keys only
* Leaves linked for range queries

üìå You usually just **explain idea**, not details.



## 9Ô∏è‚É£ Advanced Trees (Know Names + Purpose)

You don‚Äôt code these ‚Äî just **recognize** them.

| Tree           | Purpose                       |
| -- | -- |
| AVL Tree       | Self-balancing BST            |
| Red-Black Tree | Balanced, faster inserts      |
| Splay Tree     | Recently used nodes near root |
| Trie           | String searching              |
| KD-tree        | Multidimensional data         |

üëâ **Exam question**:
‚ÄúName some balanced trees and their use.‚Äù



## üìù EXAM-STYLE QUESTIONS

1. Why hashing is not suitable for range queries?
2. What is indexing?
3. Why BST is not ideal for disk-based data?
4. What is a B-tree and why it is used?
5. Difference between B-tree and B+ tree.



## ‚úÖ QUICK SELF-CHECK

Answer in your head:

* Hashing supports range queries? ‚Üí NO
* B-tree height? ‚Üí Always balanced
* Why multi-way tree? ‚Üí Fewer disk access
* Data in B+ tree stored where? ‚Üí Leaf nodes


# üìò LECTURE 10 ‚Äî Indexing & Advanced Trees

üëâ **Big idea**
Hashing is fast, but limited.
Indexing and advanced trees solve **searching large data efficiently**, especially on disk.



## 1Ô∏è‚É£ Why Hashing is NOT Enough (EXAM FAVORITE)

Even though hashing is fast (**O(1)** average), it has **limitations**.

### Limitations of Hashing:

1. ‚ùå Cannot handle **range queries**

   * Example: find students with marks between 70‚Äì80
2. ‚ùå Cannot easily find **minimum / maximum**
3. ‚ùå No sorted order
4. ‚ùå Performance depends on hash function

üëâ **Exam sentence**:

> Hashing supports only exact-match queries and does not preserve order.



## 2Ô∏è‚É£ What is Indexing? ‚≠ê‚≠ê‚≠ê

### Definition (EXAM SENTENCE)

> An index is a data structure that improves the speed of data retrieval operations on large datasets.

Think of:

* **Book index**
* **Database index**



## 3Ô∏è‚É£ What Operations Does an Index Support?

| Operation    | Time      |
|  |  |
| Insert       | O(log n)  |
| Delete       | O(log n)  |
| Exact search | O(log n)  |
| Range search | Efficient |
| Min / Max    | O(log n)  |

üëâ Indexing is **better than hashing** for many applications.



## 4Ô∏è‚É£ Linear Index

### Idea:

* Sorted list of keys
* Each key points to data location

### Problem:

* Index becomes **too large**
* Slow disk access



## 5Ô∏è‚É£ Why Not Use a Simple BST? ‚≠ê‚≠ê‚≠ê

### Problems with BST:

1. May become **unbalanced**
2. Height can be **O(n)**
3. Too many disk accesses

üëâ Disk access is **VERY slow** ‚Üí we want **short trees**



## 6Ô∏è‚É£ B-Trees ‚≠ê‚≠ê‚≠ê (IMPORTANT)

### What is a B-tree?

> A B-tree is a **height-balanced multi-way tree** designed for disk storage.



### Key Properties (MEMORIZE):

* Always **balanced**
* All leaves at **same level**
* Each node can have **many children**
* Keys inside node are **sorted**

Example:

* Order `m`
* Each node has between `‚åàm/2‚åâ` and `m` children



## 7Ô∏è‚É£ Why B-Trees Are Used in Databases?

* Very **short height**
* Fewer disk accesses
* Efficient for large datasets

üëâ **Exam sentence**:

> B-trees reduce disk access by storing multiple keys in a single node.



## 8Ô∏è‚É£ B+ Trees (Mention Only)

Difference from B-tree:

* Data stored only in **leaf nodes**
* Internal nodes store keys only
* Leaves linked for range queries

üìå You usually just **explain idea**, not details.



## 9Ô∏è‚É£ Advanced Trees (Know Names + Purpose)

You don‚Äôt code these ‚Äî just **recognize** them.

| Tree           | Purpose                       |
| -- | -- |
| AVL Tree       | Self-balancing BST            |
| Red-Black Tree | Balanced, faster inserts      |
| Splay Tree     | Recently used nodes near root |
| Trie           | String searching              |
| KD-tree        | Multidimensional data         |

üëâ **Exam question**:
‚ÄúName some balanced trees and their use.‚Äù



## üìù EXAM-STYLE QUESTIONS

1. Why hashing is not suitable for range queries?
2. What is indexing?
3. Why BST is not ideal for disk-based data?
4. What is a B-tree and why it is used?
5. Difference between B-tree and B+ tree.



## ‚úÖ QUICK SELF-CHECK

Answer in your head:

* Hashing supports range queries? ‚Üí NO
* B-tree height? ‚Üí Always balanced
* Why multi-way tree? ‚Üí Fewer disk access
* Data in B+ tree stored where? ‚Üí Leaf nodes


# üß† LECTURE 12 ‚Äî ALGORITHM DESIGN TECHNIQUES

üëâ **Big idea**
Instead of memorizing algorithms, we learn **how to design them**.

The exam will test:

* Which technique to use
* Why it works
* Where it fails



## 1Ô∏è‚É£ Why Algorithm Design Matters

### Exam-ready sentence:

> Algorithm design techniques provide systematic approaches to solve problems efficiently.



## 2Ô∏è‚É£ Three Core Design Techniques ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê

You **must know all three**:

1. **Greedy**
2. **Divide and Conquer**
3. **Dynamic Programming**



# 1Ô∏è‚É£ GREEDY ALGORITHMS ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê



## What is a Greedy Algorithm?

### Definition (EXAM SENTENCE)

> A greedy algorithm builds a solution step by step by always choosing the locally optimal choice.

üìå Makes the **best choice now**, hopes for global optimum.



## Characteristics

* Simple
* Fast
* Easy to implement
* ‚ùå Not always optimal



## Examples (MEMORIZE)

| Problem                  | Greedy? |
|  | - |
| Kruskal‚Äôs MST            | ‚úî       |
| Prim‚Äôs MST               | ‚úî       |
| Dijkstra                 | ‚úî       |
| Huffman coding           | ‚úî       |
| Coin change (some cases) | ‚ùå       |



## When Greedy Works

* Problem has **greedy-choice property**
* Optimal substructure

üëâ **Exam question**:
‚ÄúWhy greedy works for MST but not for all problems?‚Äù



## Example: Coin Change (WHY IT FAILS)

Coins: `{1, 3, 4}`
Make `6`

Greedy:

```
4 + 1 + 1 ‚Üí 3 coins
```

Optimal:

```
3 + 3 ‚Üí 2 coins
```

‚ùå Greedy fails



# 2Ô∏è‚É£ DIVIDE AND CONQUER ‚≠ê‚≠ê‚≠ê‚≠ê



## Definition

> Divide the problem into smaller subproblems, solve them recursively, and combine results.



## Steps:

1. Divide
2. Conquer
3. Combine



## Examples (MEMORIZE)

| Algorithm     | Technique        |
| - | - |
| Merge Sort    | Divide & Conquer |
| Quick Sort    | Divide & Conquer |
| Binary Search | Divide & Conquer |



## Time Complexity Pattern

Often:

```
T(n) = 2T(n/2) + O(n)
```

‚û° **O(n log n)**



## Exam Question:

‚ÄúExplain merge sort using divide and conquer.‚Äù



# 3Ô∏è‚É£ DYNAMIC PROGRAMMING (DP) ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê



## Why DP?

Used when:

* Overlapping subproblems
* Optimal substructure



## Definition (EXAM SENTENCE)

> Dynamic programming solves a problem by storing results of subproblems to avoid repeated computation.



## DP vs Greedy

| Feature    | Greedy         | DP         |
| - | -- | - |
| Choice     | Local          | Global     |
| Optimal    | Not guaranteed | Guaranteed |
| Complexity | Low            | Higher     |



## Examples (MEMORIZE)

| Problem                      | Technique |
| - |  |
| Fibonacci                    | DP        |
| Knapsack                     | DP        |
| Matrix chain                 | DP        |
| Shortest path (Bellman-Ford) | DP        |



## Fibonacci Example (KEY IDEA)

### Recursive (BAD):

* O(2‚Åø)
* Repeated work

### DP (GOOD):

* O(n)
* Store results



## Exam Question:

‚ÄúWhy dynamic programming is better than naive recursion?‚Äù



## 4Ô∏è‚É£ HOW TO CHOOSE THE RIGHT TECHNIQUE ‚≠ê‚≠ê‚≠ê

### Exam-style reasoning:

* Need fastest, simple? ‚Üí Greedy
* Problem naturally splits? ‚Üí Divide & Conquer
* Repeated subproblems? ‚Üí DP



## 5Ô∏è‚É£ CONNECTION TO YOUR COURSE ‚≠ê‚≠ê‚≠ê

| Topic          | Technique        |
| -- | - |
| Sorting        | Divide & Conquer |
| Graph MST      | Greedy           |
| Shortest path  | Greedy / DP      |
| Huffman coding | Greedy           |

üëâ **Teacher loves these connections**



## üìù EXAM-STYLE QUESTIONS (VERY LIKELY)

1. Define greedy algorithm.
2. When does greedy fail?
3. Explain divide and conquer with example.
4. What is dynamic programming?
5. Greedy vs DP comparison.
6. Which design technique is used in Kruskal‚Äôs algorithm?



## ‚úÖ FINAL SELF-CHECK (IMPORTANT)

Answer in your head:

* Greedy chooses? ‚Üí Local optimum
* DP stores? ‚Üí Subproblem results
* Merge sort technique? ‚Üí Divide & Conquer
* Kruskal uses? ‚Üí Greedy

# 2 full exam-style coding questions (with answers)

Teacher said ‚Äú2 codes‚Äù so these two are the most likely because they match our **projects** (Peak + MST).

## Coding Q1 (Peak Finding ‚Äî Project 1 style)

**Problem:** Given an integer array `A` of length `n`, find an index `i` such that `A[i]` is a **peak** (A[i] ‚â• neighbors). Return any peak index.
**Requirement:** O(log n) solution.

### Answer (C++ ‚Äî binary-search peak)

```cpp
#include <bits/stdc++.h>
using namespace std;

// Returns any peak index. Peak: A[i] >= neighbors (when they exist).
int findPeakIndex(const vector<long long>& A) {
    int n = (int)A.size();
    if (n == 0) return -1;
    if (n == 1) return 0;

    int l = 0, r = n - 1;
    while (l < r) {
        int mid = l + (r - l) / 2;

        // Compare mid with mid+1 to decide which side must contain a peak.
        if (A[mid] < A[mid + 1]) {
            // Peak must be on the right side.
            l = mid + 1;
        } else {
            // Peak is at mid or on the left side.
            r = mid;
        }
    }
    return l; // l == r is a peak index
}

int main() {
    ios::sync_with_stdio(false);
    cin.tie(nullptr);

    int n;
    cin >> n;
    vector<long long> A(n);
    for (int i = 0; i < n; i++) cin >> A[i];

    int idx = findPeakIndex(A);
    cout << "Peak index: " << idx << "\n";
    if (idx != -1) cout << "Peak value: " << A[idx] << "\n";
    return 0;
}
```

**Why it works (one line to write in exam):**
If `A[mid] < A[mid+1]`, slope goes up so a peak exists to the right; else peak exists at mid or left. This halves search each step ‚Üí **O(log n)**.



## Coding Q2 (Minimum Spanning Tree ‚Äî Kruskal, Project 3 style)

**Problem:** Given an undirected connected weighted graph (V vertices, E edges), output MST total weight and chosen edges using **Kruskal**.

### Answer (C++ ‚Äî DSU + sort edges)

```cpp
#include <bits/stdc++.h>
using namespace std;

struct DSU {
    vector<int> p, r;
    DSU(int n=0){ init(n); }
    void init(int n){
        p.resize(n);
        r.assign(n, 0);
        iota(p.begin(), p.end(), 0);
    }
    int find(int x){
        if(p[x]==x) return x;
        return p[x] = find(p[x]);
    }
    bool unite(int a, int b){
        a = find(a); b = find(b);
        if(a==b) return false;
        if(r[a] < r[b]) swap(a,b);
        p[b] = a;
        if(r[a]==r[b]) r[a]++;
        return true;
    }
};

struct Edge {
    int u, v;
    long long w;
};

int main(){
    ios::sync_with_stdio(false);
    cin.tie(nullptr);

    int V, E;
    cin >> V >> E;
    vector<Edge> edges(E);
    for(int i=0;i<E;i++){
        cin >> edges[i].u >> edges[i].v >> edges[i].w;
    }

    sort(edges.begin(), edges.end(),
         [](const Edge& a, const Edge& b){ return a.w < b.w; });

    DSU dsu(V);
    long long total = 0;
    vector<Edge> mst;

    for(const auto& e: edges){
        if(dsu.unite(e.u, e.v)){
            mst.push_back(e);
            total += e.w;
            if((int)mst.size() == V-1) break;
        }
    }

    cout << "MST total weight = " << total << "\n";
    cout << "Edges in MST:\n";
    for(const auto& e: mst){
        cout << e.u << " " << e.v << " " << e.w << "\n";
    }
    return 0;
}
```

**One-paragraph exam explanation:**
Kruskal sorts edges by weight, then scans from smallest to largest, adding an edge only if it connects two different components (checked using DSU). Stop after `V-1` edges. Time: sorting `O(E log E)`.

### What? Oh I see, you must be a Seeker from Quidditch but unfortunately I dont have nothing more to offer you.

Anyways, many many **Congratulations**, you are good to go!
